{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实战Kaggle比赛——使用Gluon对原始图像文件分类（CIFAR-10）\n",
    "\n",
    "我们在[监督学习中的一章](../chapter_supervised-learning/kaggle-gluon-kfold.md)里，以[房价预测问题](https://www.kaggle.com/c/house-prices-advanced-regression-techniques)为例，介绍了如何使用``Gluon``来实战[Kaggle比赛](https://www.kaggle.com)。\n",
    "\n",
    "我们在本章中选择了Kaggle中著名的[CIFAR-10原始图像分类问题](https://www.kaggle.com/c/cifar-10)。我们以该问题为例，为大家提供使用`Gluon`对原始图像文件进行分类的示例代码。\n",
    "\n",
    "计算机视觉一直是深度学习的主战场，请\n",
    "\n",
    "> Get your hands dirty。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Kaggle中的CIFAR-10原始图像分类问题\n",
    "\n",
    "[Kaggle](https://www.kaggle.com)是一个著名的供机器学习爱好者交流的平台。为了便于提交结果，请大家注册[Kaggle](https://www.kaggle.com)账号。然后请大家先点击[CIFAR-10原始图像分类问题](https://www.kaggle.com/c/cifar-10)了解有关本次比赛的信息。\n",
    "\n",
    "![](../img/kaggle_cifar10.png)\n",
    "\n",
    "\n",
    "\n",
    "## 整理原始数据集\n",
    "\n",
    "比赛数据分为训练数据集和测试数据集。训练集包含5万张图片。测试集包含30万张图片：其中有1万张图片用来计分，但为了防止人工标注测试集，里面另加了29万张不计分的图片。\n",
    "\n",
    "两个数据集都是png彩色图片，大小为$32\\times 32 \\times 3$。训练集一共有10类图片，分别为飞机、汽车、鸟、猫、鹿、狗、青蛙、马、船和卡车。\n",
    "\n",
    "（那么问题来了，你觉得你用肉眼能把下面100个图片正确分类吗？）\n",
    "\n",
    "![](../img/cifar10.png)\n",
    "\n",
    "\n",
    "### 下载数据集\n",
    "\n",
    "\n",
    "登录Kaggle后，数据可以从[CIFAR-10原始图像分类问题](https://www.kaggle.com/c/cifar-10)中下载。\n",
    "\n",
    "* [训练数据集train.7z下载地址](https://www.kaggle.com/c/cifar-10/download/train.7z)\n",
    "\n",
    "* [测试数据集test.7z下载地址](https://www.kaggle.com/c/cifar-10/download/test.7z)\n",
    "\n",
    "* [训练数据标签trainLabels.csv下载地址](https://www.kaggle.com/c/cifar-10/download/trainLabels.csv)\n",
    "\n",
    "\n",
    "### 解压数据集\n",
    "\n",
    "训练数据集train.7z和测试数据集test.7z都是压缩格式，下载后请解压缩。解压缩后原始数据集的路径可以如下：\n",
    "\n",
    "* ../data/kaggle_cifar10/train/[1-50000].png\n",
    "* ../data/kaggle_cifar10/test/[1-300000].png\n",
    "* ../data/kaggle_cifar10/test/trainLabels.csv\n",
    "\n",
    "为了使网页编译快一点，我们在gitrepo里仅仅存放100个训练样本（'train_tiny.zip'）和1个测试样本（'test_tiny.zip'）。执行以下代码会从git repo里解压生成小样本训练和测试数据，文件夹名称分别为'train_tiny'和'test_tiny'。训练数据标签的压缩文件将被解压成trainLabels.csv。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "1"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/kaggle_cifar10/train_tiny.zip'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-33e4a37638e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfin\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'train_tiny.zip'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test_tiny.zip'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'trainLabels.csv.zip'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/kaggle_cifar10/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mzin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m             \u001b[0mzin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/kaggle_cifar10/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/hanfeimao/anaconda/lib/python3.6/zipfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, compression, allowZip64)\u001b[0m\n\u001b[1;32m   1080\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilemode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mfilemode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodeDict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/kaggle_cifar10/train_tiny.zip'"
     ]
    }
   ],
   "source": [
    "# 如果训练下载的Kaggle的完整数据集，把下面改False\n",
    "demo = True\n",
    "if demo:\n",
    "    import zipfile\n",
    "    for fin in ['train_tiny.zip', 'test_tiny.zip', 'trainLabels.csv.zip']:\n",
    "        with zipfile.ZipFile('../data/kaggle_cifar10/' + fin, 'r') as zin:\n",
    "            zin.extractall('../data/kaggle_cifar10/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 整理数据集\n",
    "\n",
    "我们定义下面的reorg_cifar10_data函数来整理数据集。整理后，同一类图片将出现在在同一个文件夹下，便于`Gluon`稍后读取。\n",
    "\n",
    "函数中的参数如data_dir、train_dir和test_dir对应上述数据存放路径及训练和测试的图片集文件夹名称。参数label_file为训练数据标签的文件名称。参数input_dir时整理后数据集文件夹名称。参数valid_ratio是验证集占原始训练集的比重。以valid_ratio=0.1为例，由于原始训练数据有5万张图片，调参时将有4万5千张图片用于训练（整理后存放在input_dir/train）而另外5千张图片为验证集（整理后存放在input_dir/valid）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "2"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def reorg_cifar10_data(data_dir, label_file, train_dir, test_dir, input_dir, valid_ratio):\n",
    "    # 读取训练数据标签。\n",
    "    with open(os.path.join(data_dir, label_file), 'r') as f:\n",
    "        # 跳过文件头行（栏名称）。\n",
    "        lines = f.readlines()[1:]\n",
    "        tokens = [l.rstrip().split(',') for l in lines]\n",
    "        idx_label = dict(((int(idx), label) for idx, label in tokens))\n",
    "    labels = set(idx_label.values())\n",
    "\n",
    "    num_train = len(os.listdir(os.path.join(data_dir, train_dir)))\n",
    "    num_train_tuning = int(num_train * (1 - valid_ratio))\n",
    "    assert 0 < num_train_tuning < num_train\n",
    "    num_train_tuning_per_label = num_train_tuning // len(labels)\n",
    "    label_count = dict()\n",
    "\n",
    "    def mkdir_if_not_exist(path):\n",
    "        if not os.path.exists(os.path.join(*path)):\n",
    "            os.makedirs(os.path.join(*path))\n",
    "\n",
    "    # 整理训练和验证集。\n",
    "    for train_file in os.listdir(os.path.join(data_dir, train_dir)):\n",
    "        idx = int(train_file.split('.')[0])\n",
    "        label = idx_label[idx]\n",
    "        mkdir_if_not_exist([data_dir, input_dir, 'train_valid', label])\n",
    "        shutil.copy(os.path.join(data_dir, train_dir, train_file),\n",
    "                    os.path.join(data_dir, input_dir, 'train_valid', label))\n",
    "        if label not in label_count or label_count[label] < num_train_tuning_per_label:\n",
    "            mkdir_if_not_exist([data_dir, input_dir, 'train', label])\n",
    "            shutil.copy(os.path.join(data_dir, train_dir, train_file),\n",
    "                        os.path.join(data_dir, input_dir, 'train', label))\n",
    "            label_count[label] = label_count.get(label, 0) + 1\n",
    "        else:\n",
    "            mkdir_if_not_exist([data_dir, input_dir, 'valid', label])\n",
    "            shutil.copy(os.path.join(data_dir, train_dir, train_file),\n",
    "                        os.path.join(data_dir, input_dir, 'valid', label))\n",
    "\n",
    "    # 整理测试集。\n",
    "    mkdir_if_not_exist([data_dir, input_dir, 'test', 'unknown'])\n",
    "    for test_file in os.listdir(os.path.join(data_dir, test_dir)):\n",
    "        shutil.copy(os.path.join(data_dir, test_dir, test_file),\n",
    "                    os.path.join(data_dir, input_dir, 'test', 'unknown'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再次强调，为了使网页编译快一点，我们在这里仅仅使用100个训练样本和1个测试样本。训练和测试数据的文件夹名称分别为'train_tiny'和'test_tiny'。相应地，我们仅将批量大小设为1。实际训练和测试时应使用Kaggle的完整数据集。由于数据集较大，批量大小train_batch大小可设为一个较大的整数，例如128。\n",
    "\n",
    "我们将10%的训练样本作为调参时的验证集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "3"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'demo' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-9adc8350152c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mdemo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;31m# 注意：此处使用小训练集为便于网页编译。Kaggle的完整数据集应包括5万训练样本。\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtrain_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'train_tiny'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# 注意：此处使用小测试集为便于网页编译。Kaggle的完整数据集应包括30万测试样本。\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtest_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'test_tiny'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'demo' is not defined"
     ]
    }
   ],
   "source": [
    "if demo:\n",
    "    # 注意：此处使用小训练集为便于网页编译。Kaggle的完整数据集应包括5万训练样本。\n",
    "    train_dir = 'train_tiny'\n",
    "    # 注意：此处使用小测试集为便于网页编译。Kaggle的完整数据集应包括30万测试样本。\n",
    "    test_dir = 'test_tiny'\n",
    "    # 注意：此处相应使用小批量。对Kaggle的完整数据集可设较大的整数，例如128。\n",
    "    batch_size = 1\n",
    "else:\n",
    "    train_dir = 'train'\n",
    "    test_dir = 'test'\n",
    "    batch_size = 128\n",
    "\n",
    "data_dir = '../data/kaggle_cifar10'\n",
    "label_file = 'trainLabels.csv'\n",
    "input_dir = 'train_valid_test'\n",
    "valid_ratio = 0.1\n",
    "reorg_cifar10_data(data_dir, label_file, train_dir, test_dir, input_dir, valid_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用Gluon读取整理后的数据集\n",
    "\n",
    "为避免过拟合，我们在这里使用`image.CreateAugmenter`来加强数据集。例如我们设`rand_mirror=True`即可随机对每张图片做镜面反转。我们也通过`mean`和`std`对彩色图像RGB三个通道分别做[标准化](../chapter_supervised-learning/kaggle-gluon-kfold.md)。以下我们列举了该函数里的所有参数，这些参数都是可以调的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "4"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mxnet import autograd\n",
    "from mxnet import gluon\n",
    "from mxnet import image\n",
    "from mxnet import init\n",
    "from mxnet import nd\n",
    "from mxnet.gluon.data import vision\n",
    "import numpy as np\n",
    "\n",
    "def transform_train(data, label):\n",
    "    im = data.astype('float32') / 255\n",
    "    auglist = image.CreateAugmenter(data_shape=(3, 32, 32), resize=0, \n",
    "                        rand_crop=False, rand_resize=False, rand_mirror=True,\n",
    "                        mean=np.array([0.4914, 0.4822, 0.4465]), \n",
    "                        std=np.array([0.2023, 0.1994, 0.2010]), \n",
    "                        brightness=0, contrast=0, \n",
    "                        saturation=0, hue=0, \n",
    "                        pca_noise=0, rand_gray=0, inter_method=2)\n",
    "    for aug in auglist:\n",
    "        im = aug(im)\n",
    "    # 将数据格式从\"高*宽*通道\"改为\"通道*高*宽\"。\n",
    "    im = nd.transpose(im, (2,0,1))\n",
    "    return (im, nd.array([label]).asscalar().astype('float32'))\n",
    "\n",
    "# 测试时，无需对图像做标准化以外的增强数据处理。\n",
    "def transform_test(data, label):\n",
    "    im = data.astype('float32') / 255\n",
    "    auglist = image.CreateAugmenter(data_shape=(3, 32, 32), \n",
    "                        mean=np.array([0.4914, 0.4822, 0.4465]), \n",
    "                        std=np.array([0.2023, 0.1994, 0.2010]))\n",
    "    for aug in auglist:\n",
    "        im = aug(im)\n",
    "    im = nd.transpose(im, (2,0,1))\n",
    "    return (im, nd.array([label]).asscalar().astype('float32'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们可以使用`Gluon`中的`ImageFolderDataset`类来读取整理后的数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "5"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_str = data_dir + '/' + input_dir + '/'\n",
    "\n",
    "# 读取原始图像文件。flag=1说明输入图像有三个通道（彩色）。\n",
    "train_ds = vision.ImageFolderDataset(input_str + 'train', flag=1, \n",
    "                                     transform=transform_train)\n",
    "valid_ds = vision.ImageFolderDataset(input_str + 'valid', flag=1, \n",
    "                                     transform=transform_test)\n",
    "train_valid_ds = vision.ImageFolderDataset(input_str + 'train_valid', \n",
    "                                           flag=1, transform=transform_train)\n",
    "test_ds = vision.ImageFolderDataset(input_str + 'test', flag=1, \n",
    "                                     transform=transform_test)\n",
    "\n",
    "loader = gluon.data.DataLoader\n",
    "train_data = loader(train_ds, batch_size, shuffle=True, last_batch='keep')\n",
    "valid_data = loader(valid_ds, batch_size, shuffle=True, last_batch='keep')\n",
    "train_valid_data = loader(train_valid_ds, batch_size, shuffle=True, last_batch='keep')\n",
    "test_data = loader(test_ds, batch_size, shuffle=False, last_batch='keep')\n",
    "\n",
    "# 交叉熵损失函数。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mxnet.gluon.data.dataloader.DataLoader at 0x112e2a550>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 设计模型\n",
    "\n",
    "我们这里使用了[ResNet-18](resnet-gluon.md)模型。我们使用[hybridizing](../chapter_gluon-advances/hybridize.md)来提升执行效率。\n",
    "\n",
    "请注意：模型可以重新设计，参数也可以重新调整。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "6"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mxnet.gluon import nn\n",
    "from mxnet import nd\n",
    "\n",
    "class Residual(nn.HybridBlock):\n",
    "    def __init__(self, channels, same_shape=True, **kwargs):\n",
    "        super(Residual, self).__init__(**kwargs)\n",
    "        self.same_shape = same_shape\n",
    "        with self.name_scope():\n",
    "            strides = 1 if same_shape else 2\n",
    "            self.conv1 = nn.Conv2D(channels, kernel_size=3, padding=1,\n",
    "                                  strides=strides)\n",
    "            self.bn1 = nn.BatchNorm()\n",
    "            self.conv2 = nn.Conv2D(channels, kernel_size=3, padding=1)\n",
    "            self.bn2 = nn.BatchNorm()\n",
    "            if not same_shape:\n",
    "                self.conv3 = nn.Conv2D(channels, kernel_size=1,\n",
    "                                      strides=strides)\n",
    "\n",
    "    def hybrid_forward(self, F, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        if not self.same_shape:\n",
    "            x = self.conv3(x)\n",
    "        return F.relu(out + x)\n",
    "\n",
    "    \n",
    "class ResNet(nn.HybridBlock):\n",
    "    def __init__(self, num_classes, verbose=False, **kwargs):\n",
    "        super(ResNet, self).__init__(**kwargs)\n",
    "        self.verbose = verbose\n",
    "        with self.name_scope():\n",
    "            net = self.net = nn.HybridSequential()\n",
    "            # block 1\n",
    "            net.add(nn.Conv2D(channels=32, kernel_size=3, strides=1, padding=1))\n",
    "            net.add(nn.BatchNorm())\n",
    "            net.add(nn.Activation(activation='relu'))\n",
    "            # block 2\n",
    "            for _ in range(3):\n",
    "                net.add(Residual(channels=32))\n",
    "            # block 3\n",
    "            net.add(Residual(channels=64, same_shape=False))\n",
    "            for _ in range(2):\n",
    "                net.add(Residual(channels=64))\n",
    "            # block 4\n",
    "            net.add(Residual(channels=128, same_shape=False))\n",
    "            for _ in range(2):\n",
    "                net.add(Residual(channels=128))\n",
    "            # block 5\n",
    "            net.add(nn.AvgPool2D(pool_size=8))\n",
    "            net.add(nn.Flatten())\n",
    "            net.add(nn.Dense(num_classes))\n",
    "\n",
    "    def hybrid_forward(self, F, x):\n",
    "        out = x\n",
    "        for i, b in enumerate(self.net):\n",
    "            out = b(out)\n",
    "            if self.verbose:\n",
    "                print('Block %d output: %s'%(i+1, out.shape))\n",
    "        return out\n",
    "\n",
    "\n",
    "def get_net(ctx):\n",
    "    num_outputs = 10\n",
    "    net = ResNet(num_outputs)\n",
    "    net.initialize(ctx=ctx, init=init.Xavier())\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练模型并调参\n",
    "\n",
    "在[过拟合](../chapter_supervised-learning/underfit-overfit.md)中我们讲过，过度依赖训练数据集的误差来推断测试数据集的误差容易导致过拟合。由于图像分类训练时间可能较长，为了方便，我们这里不再使用K折交叉验证，而是依赖验证集的结果来调参。\n",
    "\n",
    "我们定义模型训练函数。这里我们记录每个epoch的训练时间。这有助于我们比较不同模型设计的时间成本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "7"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import utils\n",
    "\n",
    "def train(net, train_data, valid_data, num_epochs, lr, wd, ctx, lr_period, lr_decay):\n",
    "    trainer = gluon.Trainer(\n",
    "        net.collect_params(), 'sgd', {'learning_rate': lr, 'momentum': 0.9, 'wd': wd})\n",
    "\n",
    "    prev_time = datetime.datetime.now()\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0.0\n",
    "        train_acc = 0.0\n",
    "        if epoch > 0 and epoch % lr_period == 0:\n",
    "            trainer.set_learning_rate(trainer.learning_rate * lr_decay)\n",
    "        for data, label in train_data:\n",
    "            label = label.as_in_context(ctx)\n",
    "            with autograd.record():\n",
    "                output = net(data.as_in_context(ctx))\n",
    "                loss = softmax_cross_entropy(output, label)\n",
    "            loss.backward()\n",
    "            trainer.step(batch_size)\n",
    "            train_loss += nd.mean(loss).asscalar()\n",
    "            train_acc += utils.accuracy(output, label)\n",
    "        cur_time = datetime.datetime.now()\n",
    "        h, remainder = divmod((cur_time - prev_time).seconds, 3600)\n",
    "        m, s = divmod(remainder, 60)\n",
    "        time_str = \"Time %02d:%02d:%02d\" % (h, m, s)\n",
    "        if valid_data is not None:\n",
    "            valid_acc = utils.evaluate_accuracy(valid_data, net, ctx)\n",
    "            epoch_str = (\"Epoch %d. Loss: %f, Train acc %f, Valid acc %f, \"\n",
    "                         % (epoch, train_loss / len(train_data),\n",
    "                            train_acc / len(train_data), valid_acc))\n",
    "        else:\n",
    "            epoch_str = (\"Epoch %d. Loss: %f, Train acc %f, \"\n",
    "                         % (epoch, train_loss / len(train_data),\n",
    "                            train_acc / len(train_data)))\n",
    "        prev_time = cur_time\n",
    "        print(epoch_str + time_str + ', lr ' + str(trainer.learning_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下定义训练参数并训练模型。这些参数均可调。为了使网页编译快一点，我们这里将epoch数量有意设为1。事实上，epoch一般可以调大些，例如100。\n",
    "\n",
    "我们将依据验证集的结果不断优化模型设计和调整参数。依据下面的参数设置，优化算法的学习率将在每80个epoch自乘0.1。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "8"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Loss: 3.449733, Train acc 0.077778, Valid acc 0.100000, Time 00:00:02, lr 0.1\n",
      "Epoch 1. Loss: 2.569617, Train acc 0.044444, Valid acc 0.100000, Time 00:00:02, lr 0.1\n",
      "Epoch 2. Loss: 2.615988, Train acc 0.077778, Valid acc 0.100000, Time 00:00:02, lr 0.1\n",
      "Epoch 3. Loss: 2.576458, Train acc 0.066667, Valid acc 0.100000, Time 00:00:02, lr 0.1\n",
      "Epoch 4. Loss: 2.558344, Train acc 0.088889, Valid acc 0.100000, Time 00:00:02, lr 0.1\n",
      "Epoch 5. Loss: 2.553607, Train acc 0.066667, Valid acc 0.100000, Time 00:00:02, lr 0.1\n",
      "Epoch 6. Loss: 2.554197, Train acc 0.055556, Valid acc 0.100000, Time 00:00:02, lr 0.1\n",
      "Epoch 7. Loss: 2.572898, Train acc 0.066667, Valid acc 0.100000, Time 00:00:02, lr 0.1\n",
      "Epoch 8. Loss: 2.623793, Train acc 0.066667, Valid acc 0.100000, Time 00:00:02, lr 0.1\n",
      "Epoch 9. Loss: 2.506543, Train acc 0.088889, Valid acc 0.100000, Time 00:00:02, lr 0.1\n",
      "Epoch 10. Loss: 2.596478, Train acc 0.066667, Valid acc 0.100000, Time 00:00:02, lr 0.1\n",
      "Epoch 11. Loss: 2.561298, Train acc 0.066667, Valid acc 0.100000, Time 00:00:02, lr 0.1\n",
      "Epoch 12. Loss: 2.530611, Train acc 0.055556, Valid acc 0.100000, Time 00:00:02, lr 0.1\n",
      "Epoch 13. Loss: 2.577543, Train acc 0.066667, Valid acc 0.100000, Time 00:00:02, lr 0.1\n",
      "Epoch 14. Loss: 2.565693, Train acc 0.100000, Valid acc 0.100000, Time 00:00:02, lr 0.1\n",
      "Epoch 15. Loss: 2.498068, Train acc 0.100000, Valid acc 0.200000, Time 00:00:02, lr 0.1\n",
      "Epoch 16. Loss: 2.636565, Train acc 0.066667, Valid acc 0.100000, Time 00:00:02, lr 0.1\n",
      "Epoch 17. Loss: 2.573570, Train acc 0.055556, Valid acc 0.100000, Time 00:00:02, lr 0.1\n",
      "Epoch 18. Loss: 2.546142, Train acc 0.122222, Valid acc 0.100000, Time 00:00:02, lr 0.1\n",
      "Epoch 19. Loss: 2.554704, Train acc 0.066667, Valid acc 0.100000, Time 00:00:02, lr 0.1\n",
      "Epoch 20. Loss: 2.541398, Train acc 0.066667, Valid acc 0.100000, Time 00:00:02, lr 0.1\n",
      "Epoch 21. Loss: 2.783537, Train acc 0.022222, Valid acc 0.100000, Time 00:00:02, lr 0.1\n",
      "Epoch 22. Loss: 2.590449, Train acc 0.077778, Valid acc 0.100000, Time 00:00:02, lr 0.1\n",
      "Epoch 23. Loss: 2.486649, Train acc 0.122222, Valid acc 0.100000, Time 00:00:02, lr 0.1\n",
      "Epoch 24. Loss: 2.574619, Train acc 0.055556, Valid acc 0.100000, Time 00:00:02, lr 0.1\n",
      "Epoch 25. Loss: 2.562933, Train acc 0.055556, Valid acc 0.000000, Time 00:00:02, lr 0.1\n",
      "Epoch 26. Loss: 2.563420, Train acc 0.088889, Valid acc 0.000000, Time 00:00:02, lr 0.1\n",
      "Epoch 27. Loss: 2.593923, Train acc 0.055556, Valid acc 0.100000, Time 00:00:02, lr 0.1\n",
      "Epoch 28. Loss: 2.567919, Train acc 0.088889, Valid acc 0.100000, Time 00:00:02, lr 0.1\n",
      "Epoch 29. Loss: 2.597491, Train acc 0.066667, Valid acc 0.100000, Time 00:00:02, lr 0.1\n",
      "Epoch 30. Loss: 2.546649, Train acc 0.100000, Valid acc 0.100000, Time 00:00:02, lr 0.1\n",
      "Epoch 31. Loss: 2.527356, Train acc 0.077778, Valid acc 0.100000, Time 00:00:02, lr 0.1\n",
      "Epoch 32. Loss: 2.555183, Train acc 0.111111, Valid acc 0.100000, Time 00:00:02, lr 0.1\n",
      "Epoch 33. Loss: 2.526498, Train acc 0.066667, Valid acc 0.100000, Time 00:00:02, lr 0.1\n",
      "Epoch 34. Loss: 2.616267, Train acc 0.055556, Valid acc 0.000000, Time 00:00:02, lr 0.1\n",
      "Epoch 35. Loss: 2.549395, Train acc 0.088889, Valid acc 0.100000, Time 00:00:02, lr 0.1\n",
      "Epoch 36. Loss: 2.611911, Train acc 0.044444, Valid acc 0.100000, Time 00:00:02, lr 0.1\n",
      "Epoch 37. Loss: 2.599696, Train acc 0.100000, Valid acc 0.100000, Time 00:00:02, lr 0.1\n",
      "Epoch 38. Loss: 2.557523, Train acc 0.066667, Valid acc 0.100000, Time 00:00:02, lr 0.1\n",
      "Epoch 39. Loss: 2.574084, Train acc 0.077778, Valid acc 0.100000, Time 00:00:02, lr 0.1\n",
      "Epoch 40. Loss: 2.644888, Train acc 0.055556, Valid acc 0.100000, Time 00:00:02, lr 0.1\n",
      "Epoch 41. Loss: 2.575826, Train acc 0.100000, Valid acc 0.100000, Time 00:00:02, lr 0.1\n",
      "Epoch 42. Loss: 2.603819, Train acc 0.066667, Valid acc 0.100000, Time 00:00:02, lr 0.1\n",
      "Epoch 43. Loss: 2.587429, Train acc 0.044444, Valid acc 0.100000, Time 00:00:02, lr 0.1\n",
      "Epoch 44. Loss: 2.533705, Train acc 0.077778, Valid acc 0.100000, Time 00:00:02, lr 0.1\n",
      "Epoch 45. Loss: 2.496655, Train acc 0.055556, Valid acc 0.100000, Time 00:00:02, lr 0.1\n",
      "Epoch 46. Loss: 2.590765, Train acc 0.066667, Valid acc 0.100000, Time 00:00:02, lr 0.1\n",
      "Epoch 47. Loss: 2.565125, Train acc 0.066667, Valid acc 0.100000, Time 00:00:02, lr 0.1\n",
      "Epoch 48. Loss: 2.571593, Train acc 0.088889, Valid acc 0.100000, Time 00:00:02, lr 0.1\n",
      "Epoch 49. Loss: 2.652454, Train acc 0.055556, Valid acc 0.100000, Time 00:00:02, lr 0.1\n"
     ]
    }
   ],
   "source": [
    "ctx = utils.try_gpu()\n",
    "num_epochs = 50\n",
    "learning_rate = 0.1\n",
    "weight_decay = 5e-4\n",
    "lr_period = 80\n",
    "lr_decay = 0.1\n",
    "\n",
    "net = get_net(ctx)\n",
    "net.hybridize()\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, \n",
    "      weight_decay, ctx, lr_period, lr_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对测试集分类\n",
    "\n",
    "当得到一组满意的模型设计和参数后，我们使用全部训练数据集（含验证集）重新训练模型，并对测试集分类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "9"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Loss: 3.526920, Train acc 0.090000, Time 00:00:03, lr 0.1\n",
      "Epoch 1. Loss: 2.493337, Train acc 0.090000, Time 00:00:02, lr 0.1\n",
      "Epoch 2. Loss: 2.612474, Train acc 0.080000, Time 00:00:02, lr 0.1\n",
      "Epoch 3. Loss: 2.602166, Train acc 0.070000, Time 00:00:03, lr 0.1\n",
      "Epoch 4. Loss: 2.614938, Train acc 0.050000, Time 00:00:02, lr 0.1\n",
      "Epoch 5. Loss: 2.573865, Train acc 0.070000, Time 00:00:02, lr 0.1\n",
      "Epoch 6. Loss: 2.582180, Train acc 0.050000, Time 00:00:02, lr 0.1\n",
      "Epoch 7. Loss: 2.561596, Train acc 0.100000, Time 00:00:02, lr 0.1\n",
      "Epoch 8. Loss: 2.532336, Train acc 0.070000, Time 00:00:03, lr 0.1\n",
      "Epoch 9. Loss: 2.557090, Train acc 0.100000, Time 00:00:02, lr 0.1\n",
      "Epoch 10. Loss: 2.514271, Train acc 0.110000, Time 00:00:02, lr 0.1\n",
      "Epoch 11. Loss: 2.505009, Train acc 0.100000, Time 00:00:02, lr 0.1\n",
      "Epoch 12. Loss: 2.509915, Train acc 0.070000, Time 00:00:02, lr 0.1\n",
      "Epoch 13. Loss: 2.551907, Train acc 0.120000, Time 00:00:02, lr 0.1\n",
      "Epoch 14. Loss: 2.571043, Train acc 0.100000, Time 00:00:02, lr 0.1\n",
      "Epoch 15. Loss: 2.602217, Train acc 0.090000, Time 00:00:02, lr 0.1\n",
      "Epoch 16. Loss: 2.521768, Train acc 0.130000, Time 00:00:02, lr 0.1\n",
      "Epoch 17. Loss: 2.506176, Train acc 0.110000, Time 00:00:02, lr 0.1\n",
      "Epoch 18. Loss: 2.465236, Train acc 0.100000, Time 00:00:02, lr 0.1\n",
      "Epoch 19. Loss: 2.611644, Train acc 0.070000, Time 00:00:02, lr 0.1\n",
      "Epoch 20. Loss: 2.512405, Train acc 0.120000, Time 00:00:02, lr 0.1\n",
      "Epoch 21. Loss: 2.450276, Train acc 0.150000, Time 00:00:03, lr 0.1\n",
      "Epoch 22. Loss: 2.574099, Train acc 0.070000, Time 00:00:03, lr 0.1\n",
      "Epoch 23. Loss: 2.549134, Train acc 0.130000, Time 00:00:02, lr 0.1\n",
      "Epoch 24. Loss: 2.552421, Train acc 0.100000, Time 00:00:02, lr 0.1\n",
      "Epoch 25. Loss: 2.553861, Train acc 0.090000, Time 00:00:02, lr 0.1\n",
      "Epoch 26. Loss: 2.542161, Train acc 0.090000, Time 00:00:03, lr 0.1\n",
      "Epoch 27. Loss: 2.495851, Train acc 0.070000, Time 00:00:03, lr 0.1\n",
      "Epoch 28. Loss: 2.498933, Train acc 0.090000, Time 00:00:03, lr 0.1\n",
      "Epoch 29. Loss: 2.435749, Train acc 0.150000, Time 00:00:02, lr 0.1\n",
      "Epoch 30. Loss: 2.590139, Train acc 0.120000, Time 00:00:02, lr 0.1\n",
      "Epoch 31. Loss: 2.522952, Train acc 0.110000, Time 00:00:03, lr 0.1\n",
      "Epoch 32. Loss: 2.516521, Train acc 0.080000, Time 00:00:04, lr 0.1\n",
      "Epoch 33. Loss: 2.495204, Train acc 0.140000, Time 00:00:04, lr 0.1\n",
      "Epoch 34. Loss: 2.708680, Train acc 0.070000, Time 00:00:04, lr 0.1\n",
      "Epoch 35. Loss: 2.548983, Train acc 0.110000, Time 00:00:03, lr 0.1\n",
      "Epoch 36. Loss: 2.578476, Train acc 0.070000, Time 00:00:03, lr 0.1\n",
      "Epoch 37. Loss: 2.502104, Train acc 0.080000, Time 00:00:02, lr 0.1\n",
      "Epoch 38. Loss: 4.482195, Train acc 0.060000, Time 00:00:02, lr 0.1\n",
      "Epoch 39. Loss: 2.568310, Train acc 0.080000, Time 00:00:03, lr 0.1\n",
      "Epoch 40. Loss: 2.566467, Train acc 0.080000, Time 00:00:03, lr 0.1\n",
      "Epoch 41. Loss: 2.624559, Train acc 0.060000, Time 00:00:03, lr 0.1\n",
      "Epoch 42. Loss: 2.547864, Train acc 0.070000, Time 00:00:03, lr 0.1\n",
      "Epoch 43. Loss: 2.565917, Train acc 0.030000, Time 00:00:03, lr 0.1\n",
      "Epoch 44. Loss: 2.573413, Train acc 0.070000, Time 00:00:04, lr 0.1\n",
      "Epoch 45. Loss: 2.586917, Train acc 0.080000, Time 00:00:03, lr 0.1\n",
      "Epoch 46. Loss: 2.489916, Train acc 0.110000, Time 00:00:03, lr 0.1\n",
      "Epoch 47. Loss: 2.612275, Train acc 0.060000, Time 00:00:03, lr 0.1\n",
      "Epoch 48. Loss: 2.550911, Train acc 0.090000, Time 00:00:03, lr 0.1\n",
      "Epoch 49. Loss: 2.571467, Train acc 0.120000, Time 00:00:03, lr 0.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "net = get_net(ctx)\n",
    "net.hybridize()\n",
    "train(net, train_valid_data, None, num_epochs, learning_rate, \n",
    "      weight_decay, ctx, lr_period, lr_decay)\n",
    "\n",
    "preds = []\n",
    "for data, label in test_data:\n",
    "    output = net(data.as_in_context(ctx))\n",
    "    preds.extend(output.argmax(axis=1).astype(int).asnumpy())\n",
    "\n",
    "sorted_ids = list(range(1, len(test_ds) + 1))\n",
    "sorted_ids.sort(key = lambda x:str(x))\n",
    "\n",
    "df = pd.DataFrame({'id': sorted_ids, 'label': preds})\n",
    "df['label'] = df['label'].apply(lambda x: train_valid_ds.synsets[x])\n",
    "df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述代码执行完会生成一个`submission.csv`的文件用于在Kaggle上提交。这是Kaggle要求的提交格式。这时我们可以在Kaggle上把对测试集分类的结果提交并查看分类准确率。你需要登录Kaggle网站，打开[CIFAR-10原始图像分类问题](https://www.kaggle.com/c/cifar-10)，并点击下方右侧`Late Submission`按钮。\n",
    "\n",
    "![](../img/kaggle_submit3.png)\n",
    "\n",
    "\n",
    "请点击下方`Upload Submission File`选择需要提交的预测结果。然后点击下方的`Make Submission`按钮就可以查看结果啦！\n",
    "\n",
    "![](../img/kaggle_submit4.png)\n",
    "\n",
    "\n",
    "\n",
    "## 作业（[汇报作业和查看其他小伙伴作业](https://discuss.gluon.ai/t/topic/1545/)）：\n",
    "\n",
    "* 使用Kaggle完整CIFAR-10数据集，把batch_size和num_epochs分别改为128和100，可以在Kaggle上拿到什么样的准确率和名次？\n",
    "* 如果不使用增强数据的方法能拿到什么样的准确率？\n",
    "* 你还有什么其他办法可以继续改进模型和参数？小伙伴们都期待你的分享。\n",
    "\n",
    "**吐槽和讨论欢迎点**[这里](https://discuss.gluon.ai/t/topic/1545/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
