{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage.util.montage import montage2d\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "base_path = os.path.join('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n"
     ]
    }
   ],
   "source": [
    "#Load data\n",
    "train = pd.read_json(\"../data/train.json\")\n",
    "test = pd.read_json(\"../data/test.json\")\n",
    "train.inc_angle = train.inc_angle.replace('na', 0)\n",
    "train.inc_angle = train.inc_angle.astype(float).fillna(0.0)\n",
    "print(\"done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train data\n",
    "x_band1 = np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in train[\"band_1\"]])\n",
    "x_band2 = np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in train[\"band_2\"]])\n",
    "X_train = np.concatenate([x_band1[:, :, :, np.newaxis]\n",
    "                          ,x_band2[:, :, :, np.newaxis]\n",
    "                         ,((x_band1+x_band1)/2)[:, :, :, np.newaxis]], axis=-1)\n",
    "X_angle_train = np.array(train.inc_angle)\n",
    "y_train = np.array(train[\"is_iceberg\"])\n",
    "\n",
    "# Test data\n",
    "x_band1 = np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in test[\"band_1\"]])\n",
    "x_band2 = np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in test[\"band_2\"]])\n",
    "X_test = np.concatenate([x_band1[:, :, :, np.newaxis]\n",
    "                          , x_band2[:, :, :, np.newaxis]\n",
    "                         , ((x_band1+x_band1)/2)[:, :, :, np.newaxis]], axis=-1)\n",
    "X_angle_test = np.array(test.inc_angle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hanfeimao/anaconda/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2010: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_valid, X_angle_train, X_angle_valid, y_train, y_valid = train_test_split(X_train\n",
    "                    ,X_angle_train, y_train, random_state=123, train_size=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mxnet import ndarray as nd\n",
    "from mxnet import autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mxnet.gluon import nn\n",
    "from mxnet import init\n",
    "\n",
    "\n",
    "class Residual(nn.HybridBlock):\n",
    "    def __init__(self, channels, same_shape=True, **kwargs):\n",
    "        super(Residual, self).__init__(**kwargs)\n",
    "        self.same_shape = same_shape\n",
    "        with self.name_scope():\n",
    "            strides = 1 if same_shape else 2\n",
    "            self.conv1 = nn.Conv2D(channels, kernel_size=3, padding=1, strides=strides)\n",
    "            self.bn1 = nn.BatchNorm()\n",
    "            self.conv2 = nn.Conv2D(channels, kernel_size=3, padding=1)\n",
    "            self.bn2 = nn.BatchNorm()\n",
    "            if not same_shape:\n",
    "                self.conv3 = nn.Conv2D(channels, kernel_size=1, strides=strides)\n",
    "\n",
    "    def hybrid_forward(self, F, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        if not self.same_shape:\n",
    "            x = self.conv3(x)\n",
    "        return F.relu(out + x)\n",
    "\n",
    "\n",
    "class ResNet(nn.HybridBlock):\n",
    "    def __init__(self, num_classes, verbose=False, **kwargs):\n",
    "        super(ResNet, self).__init__(**kwargs)\n",
    "        self.verbose = verbose\n",
    "        with self.name_scope():\n",
    "            net = self.net = nn.HybridSequential()\n",
    "            # block 1\n",
    "            net.add(nn.Conv2D(channels=32, kernel_size=3, strides=1, padding=1))\n",
    "            net.add(nn.BatchNorm())\n",
    "            net.add(nn.Activation(activation='relu'))\n",
    "            # block 2\n",
    "            for _ in range(3):\n",
    "                net.add(Residual(channels=32))\n",
    "            # block 3\n",
    "            net.add(Residual(channels=64, same_shape=False))\n",
    "            for _ in range(2):\n",
    "                net.add(Residual(channels=64))\n",
    "            # block 4\n",
    "            net.add(Residual(channels=128, same_shape=False))\n",
    "            for _ in range(2):\n",
    "                net.add(Residual(channels=128))\n",
    "            # block 5\n",
    "            net.add(nn.AvgPool2D(pool_size=8))\n",
    "            net.add(nn.Flatten())\n",
    "            net.add(nn.Dense(num_classes))\n",
    "\n",
    "    def hybrid_forward(self, F, x):\n",
    "        out = x\n",
    "        for i, b in enumerate(self.net):\n",
    "            out = b(out)\n",
    "            if self.verbose:\n",
    "                print('Block %d output: %s'%(i+1, out.shape))\n",
    "        return out\n",
    "\n",
    "\n",
    "def get_net(ctx):\n",
    "    num_outputs = 2\n",
    "    net = ResNet(num_outputs)\n",
    "    net.initialize(ctx=ctx, init=init.Xavier())\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cpu(0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mxnet as mx\n",
    "try:\n",
    "    ctx = mx.gpu()\n",
    "    _ = nd.zeros((1,), ctx=ctx)\n",
    "except:\n",
    "    ctx = mx.cpu()\n",
    "ctx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mxnet import autograd\n",
    "from mxnet import gluon\n",
    "from mxnet import image\n",
    "from mxnet import init\n",
    "from mxnet import nd\n",
    "from mxnet.gluon.data import vision\n",
    "import numpy as np\n",
    "\n",
    "def transform_train(data, label):\n",
    "    im = data.astype('float32') / 255\n",
    "    auglist = image.CreateAugmenter(data_shape=(3, 32, 32), resize=0, \n",
    "                        rand_crop=False, rand_resize=False, rand_mirror=True,\n",
    "                        mean=np.array([0.4914, 0.4822, 0.4465]), \n",
    "                        std=np.array([0.2023, 0.1994, 0.2010]), \n",
    "                        brightness=0, contrast=0, \n",
    "                        saturation=0, hue=0, \n",
    "                        pca_noise=0, rand_gray=0, inter_method=2)\n",
    "    for aug in auglist:\n",
    "        im = aug(im)\n",
    "    # 将数据格式从\"高*宽*通道\"改为\"通道*高*宽\"。\n",
    "    im = nd.transpose(im, (2,0,1))\n",
    "    return (im, nd.array([label]).asscalar().astype('float32'))\n",
    "\n",
    "# 测试时，无需对图像做标准化以外的增强数据处理。\n",
    "def transform_test(data, label):\n",
    "    im = data.astype('float32') / 255\n",
    "    auglist = image.CreateAugmenter(data_shape=(3, 32, 32), \n",
    "                        mean=np.array([0.4914, 0.4822, 0.4465]), \n",
    "                        std=np.array([0.2023, 0.1994, 0.2010]))\n",
    "    for aug in auglist:\n",
    "        im = aug(im)\n",
    "    im = nd.transpose(im, (2,0,1))\n",
    "    return (im, nd.array([label]).asscalar().astype('float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1604, 75, 75, 2)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#data, label in one list\n",
    "ib_train = [(nd.transpose(nd.array(train_images[x]),(2,0,1)),\n",
    "             train_df['is_iceberg'][x])\n",
    "            for x in range(len(train_df['is_iceberg']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_images = [nd.transpose(nd.array(train_images[x]),(2,0,1)) for x in range(len(train_df['is_iceberg']))]\n",
    "test_images = [nd.transpose(nd.array(test_images[x]),(2,0,1)) for x in range(len(test_df['id']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_images, np.asarray(train_df['is_iceberg']), \n",
    "                                                    test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_ds = [(X_train[i],y_train[i]) for i in range(len(y_train))]\n",
    "valid_ds = [(X_test[i],y_test[i]) for i in range(len(y_test))]\n",
    "train_valid_ds = [(train_images[i], np.asarray(train_df['is_iceberg'][i])) for i in range(len(train_df['is_iceberg']))]\n",
    "test_ds = [(test_images[i], 0) for i in range(len(test_df['id']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mxnet import gluon\n",
    "batch_size = 128\n",
    "loader = gluon.data.DataLoader\n",
    "train_data = loader(train_ds, batch_size, shuffle=True, last_batch='keep')\n",
    "valid_data = loader(valid_ds, batch_size, shuffle=True, last_batch='keep')\n",
    "train_valid_data = loader(train_valid_ds, batch_size, shuffle=True, last_batch='keep')\n",
    "test_data = loader(test_ds, batch_size, shuffle = False, last_batch='keep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "def train(net, train_data, valid_data, num_epochs, lr, wd, ctx, lr_period, lr_decay):\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr, 'momentum': 0.9, 'wd': wd})\n",
    "\n",
    "    prev_time = datetime.datetime.now()\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0.0\n",
    "        train_acc = 0.0\n",
    "        if epoch > 0 and epoch % lr_period == 0:\n",
    "            trainer.set_learning_rate(trainer.learning_rate * lr_decay)\n",
    "        for data, label in train_data:\n",
    "            label = label.as_in_context(ctx)\n",
    "            with autograd.record():\n",
    "                output = net(data.as_in_context(ctx))\n",
    "                loss = softmax_cross_entropy(output, label)\n",
    "            loss.backward()\n",
    "            trainer.step(batch_size)\n",
    "            train_loss += nd.mean(loss).asscalar()\n",
    "            train_acc += accuracy(output, label)\n",
    "        cur_time = datetime.datetime.now()\n",
    "        h, remainder = divmod((cur_time - prev_time).seconds, 3600)\n",
    "        m, s = divmod(remainder, 60)\n",
    "        time_str = \"Time %02d:%02d:%02d\" % (h, m, s)\n",
    "        if valid_data is not None:\n",
    "            valid_acc = evaluate_accuracy(valid_data, net, ctx)\n",
    "            epoch_str = (\"Epoch %d. Loss: %f, Train acc %f, Valid acc %f, \"\n",
    "                         % (epoch, train_loss / len(train_data),\n",
    "                            train_acc / len(train_data), valid_acc))\n",
    "        else:\n",
    "            epoch_str = (\"Epoch %d. Loss: %f, Train acc %f, \"\n",
    "                         % (epoch, train_loss / len(train_data),\n",
    "                            train_acc / len(train_data)))\n",
    "        prev_time = cur_time\n",
    "        print(epoch_str + time_str + ', lr ' + str(trainer.learning_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(output, label):\n",
    "    return np.mean(output.asnumpy().argmax(axis=1)==label.asnumpy())\n",
    "\n",
    "def evaluate_accuracy(data_iterator, net, ctx=[mx.cpu()]):\n",
    "    if isinstance(ctx, mx.Context):\n",
    "        ctx = [ctx]\n",
    "    acc = nd.array([0])\n",
    "    n = 0.\n",
    "    if isinstance(data_iterator, mx.io.MXDataIter):\n",
    "        data_iterator.reset()\n",
    "    for batch in data_iterator:\n",
    "        data, label, batch_size = _get_batch(batch, ctx)\n",
    "        for X, y in zip(data, label):\n",
    "            acc += nd.array([np.sum(net(X).asnumpy().argmax(axis=1)==y.asnumpy())]).copyto(mx.cpu())\n",
    "        acc.wait_to_read() # don't push too many operators into backend\n",
    "        n += batch_size\n",
    "    return acc.asscalar() / n\n",
    "\n",
    "def _get_batch(batch, ctx):\n",
    "    \"\"\"return data and label on ctx\"\"\"\n",
    "    if isinstance(batch, mx.io.DataBatch):\n",
    "        data = batch.data[0]\n",
    "        label = batch.label[0]\n",
    "    else:\n",
    "        data, label = batch\n",
    "    return (gluon.utils.split_and_load(data, ctx),\n",
    "            gluon.utils.split_and_load(label, ctx),\n",
    "            data.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Loss: 3.282347, Train acc 0.646307, Valid acc 0.489097, Time 00:03:01, lr 0.1\n",
      "Epoch 1. Loss: 0.557262, Train acc 0.706676, Valid acc 0.489097, Time 00:03:01, lr 0.1\n",
      "Epoch 2. Loss: 0.542793, Train acc 0.658144, Valid acc 0.616822, Time 00:02:59, lr 0.1\n",
      "Epoch 3. Loss: 0.539821, Train acc 0.673769, Valid acc 0.691589, Time 00:02:58, lr 0.1\n",
      "Epoch 4. Loss: 0.514388, Train acc 0.709754, Valid acc 0.728972, Time 00:02:59, lr 0.1\n",
      "Epoch 5. Loss: 0.495658, Train acc 0.709754, Valid acc 0.738318, Time 00:04:56, lr 0.1\n",
      "Epoch 6. Loss: 0.485165, Train acc 0.745739, Valid acc 0.741433, Time 00:02:58, lr 0.1\n",
      "Epoch 7. Loss: 0.461456, Train acc 0.746449, Valid acc 0.744548, Time 00:02:58, lr 0.1\n",
      "Epoch 8. Loss: 0.473991, Train acc 0.724669, Valid acc 0.738318, Time 00:02:58, lr 0.1\n",
      "Epoch 9. Loss: 0.470691, Train acc 0.714725, Valid acc 0.725857, Time 00:02:59, lr 0.1\n",
      "Epoch 10. Loss: 0.483553, Train acc 0.742898, Valid acc 0.763240, Time 00:03:00, lr 0.1\n",
      "Epoch 11. Loss: 0.469781, Train acc 0.717566, Valid acc 0.741433, Time 00:05:43, lr 0.1\n",
      "Epoch 12. Loss: 0.465604, Train acc 0.727509, Valid acc 0.741433, Time 00:03:02, lr 0.1\n",
      "Epoch 13. Loss: 1.051161, Train acc 0.726799, Valid acc 0.747664, Time 00:03:30, lr 0.1\n",
      "Epoch 14. Loss: 0.461795, Train acc 0.730350, Valid acc 0.732087, Time 00:03:56, lr 0.1\n",
      "Epoch 15. Loss: 0.412474, Train acc 0.762784, Valid acc 0.741433, Time 00:03:19, lr 0.1\n",
      "Epoch 16. Loss: 0.427741, Train acc 0.734612, Valid acc 0.772586, Time 00:03:08, lr 0.1\n",
      "Epoch 17. Loss: 0.394220, Train acc 0.775568, Valid acc 0.750779, Time 00:03:36, lr 0.1\n",
      "Epoch 18. Loss: 0.388562, Train acc 0.801136, Valid acc 0.760125, Time 00:03:04, lr 0.1\n",
      "Epoch 19. Loss: 0.432552, Train acc 0.725616, Valid acc 0.635514, Time 00:03:04, lr 0.1\n",
      "Epoch 20. Loss: 0.669797, Train acc 0.646780, Valid acc 0.772586, Time 00:03:01, lr 0.010000000000000002\n",
      "Epoch 21. Loss: 0.367543, Train acc 0.791193, Valid acc 0.757009, Time 00:03:06, lr 0.010000000000000002\n",
      "Epoch 22. Loss: 0.404532, Train acc 0.775095, Valid acc 0.778816, Time 00:03:26, lr 0.010000000000000002\n",
      "Epoch 23. Loss: 0.571964, Train acc 0.816998, Valid acc 0.788162, Time 00:03:22, lr 0.010000000000000002\n",
      "Epoch 24. Loss: 0.411559, Train acc 0.819129, Valid acc 0.806854, Time 00:03:04, lr 0.010000000000000002\n",
      "Epoch 25. Loss: 0.344792, Train acc 0.846591, Valid acc 0.797508, Time 00:03:01, lr 0.010000000000000002\n",
      "Epoch 26. Loss: 0.440812, Train acc 0.765625, Valid acc 0.809969, Time 00:03:01, lr 0.010000000000000002\n",
      "Epoch 27. Loss: 0.379089, Train acc 0.824100, Valid acc 0.788162, Time 00:03:02, lr 0.010000000000000002\n",
      "Epoch 28. Loss: 0.330702, Train acc 0.831913, Valid acc 0.806854, Time 00:03:02, lr 0.010000000000000002\n",
      "Epoch 29. Loss: 0.309204, Train acc 0.862216, Valid acc 0.797508, Time 00:03:05, lr 0.010000000000000002\n",
      "Epoch 30. Loss: 0.320929, Train acc 0.834754, Valid acc 0.806854, Time 00:03:04, lr 0.010000000000000002\n",
      "Epoch 31. Loss: 0.334732, Train acc 0.840436, Valid acc 0.831776, Time 00:03:04, lr 0.010000000000000002\n",
      "Epoch 32. Loss: 0.300180, Train acc 0.867898, Valid acc 0.806854, Time 00:03:02, lr 0.010000000000000002\n",
      "Epoch 33. Loss: 0.284363, Train acc 0.879261, Valid acc 0.838006, Time 00:03:04, lr 0.010000000000000002\n",
      "Epoch 34. Loss: 0.319840, Train acc 0.855350, Valid acc 0.816199, Time 00:03:07, lr 0.010000000000000002\n",
      "Epoch 35. Loss: 0.536133, Train acc 0.856061, Valid acc 0.850467, Time 00:03:05, lr 0.010000000000000002\n",
      "Epoch 36. Loss: 0.317967, Train acc 0.820786, Valid acc 0.847352, Time 00:03:01, lr 0.010000000000000002\n",
      "Epoch 37. Loss: 0.267936, Train acc 0.887074, Valid acc 0.847352, Time 00:02:59, lr 0.010000000000000002\n",
      "Epoch 38. Loss: 0.236489, Train acc 0.897727, Valid acc 0.866044, Time 00:02:58, lr 0.010000000000000002\n",
      "Epoch 39. Loss: 0.311773, Train acc 0.869555, Valid acc 0.822430, Time 00:03:07, lr 0.010000000000000002\n",
      "Epoch 40. Loss: 0.259550, Train acc 0.875237, Valid acc 0.853583, Time 00:03:04, lr 0.0010000000000000002\n",
      "Epoch 41. Loss: 0.258195, Train acc 0.870265, Valid acc 0.856698, Time 00:03:00, lr 0.0010000000000000002\n",
      "Epoch 42. Loss: 0.219040, Train acc 0.908381, Valid acc 0.844237, Time 00:03:00, lr 0.0010000000000000002\n",
      "Epoch 43. Loss: 0.221427, Train acc 0.904119, Valid acc 0.841121, Time 00:02:59, lr 0.0010000000000000002\n",
      "Epoch 44. Loss: 0.302974, Train acc 0.882339, Valid acc 0.844237, Time 00:03:01, lr 0.0010000000000000002\n",
      "Epoch 45. Loss: 0.216397, Train acc 0.907670, Valid acc 0.850467, Time 00:03:00, lr 0.0010000000000000002\n",
      "Epoch 46. Loss: 0.327467, Train acc 0.846354, Valid acc 0.859813, Time 00:03:07, lr 0.0010000000000000002\n",
      "Epoch 47. Loss: 0.212587, Train acc 0.911932, Valid acc 0.844237, Time 00:03:07, lr 0.0010000000000000002\n",
      "Epoch 48. Loss: 0.268508, Train acc 0.884470, Valid acc 0.838006, Time 00:03:02, lr 0.0010000000000000002\n",
      "Epoch 49. Loss: 0.209097, Train acc 0.906250, Valid acc 0.869159, Time 00:03:03, lr 0.0010000000000000002\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "learning_rate = 0.1\n",
    "weight_decay = 1e-4\n",
    "lr_period = 20\n",
    "lr_decay = 0.1\n",
    "\n",
    "net = get_net(ctx)\n",
    "net.hybridize()\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, weight_decay, ctx, lr_period, lr_decay)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds = []\n",
    "for data, label in test_data:\n",
    "    output = net(data.as_in_context(ctx))\n",
    "    preds.extend(output.asnumpy().argmax(axis=1))\n",
    "    \n",
    "df = pd.DataFrame({'id': test_df['id'], 'is_iceberg':preds})\n",
    "df['id'] =df['id'].astype(str)\n",
    "df.to_csv('../submit/submission2.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'06565646' in list(df['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mask = df.is_iceberg == 0\n",
    "column_name = 'is_iceberg'\n",
    "df.loc[mask, column_name] = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mask = df.is_iceberg == 1\n",
    "df.loc[mask, column_name] = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.to_csv('../submit/submission1.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
