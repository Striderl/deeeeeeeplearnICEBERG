{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage.util.montage import montage2d\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "base_path = os.path.join('data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n"
     ]
    }
   ],
   "source": [
    "#Load data\n",
    "train = pd.read_json(\"../data/train.json\")\n",
    "test = pd.read_json(\"../data/test.json\")\n",
    "train.inc_angle = train.inc_angle.replace('na', 0)\n",
    "train.inc_angle = train.inc_angle.astype(float).fillna(0.0)\n",
    "print(\"done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train data\n",
    "x_band1 = np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in train[\"band_1\"]])\n",
    "x_band2 = np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in train[\"band_2\"]])\n",
    "X_train = np.concatenate([x_band1[:, :, :, np.newaxis]\n",
    "                          ,x_band2[:, :, :, np.newaxis]\n",
    "                         ,((x_band1+x_band2)/2)[:, :, :, np.newaxis]], axis=-1)\n",
    "X_angle_train = np.array(train.inc_angle)\n",
    "y_train = np.array(train[\"is_iceberg\"])\n",
    "\n",
    "# Test data\n",
    "x_band1 = np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in test[\"band_1\"]])\n",
    "x_band2 = np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in test[\"band_2\"]])\n",
    "X_test = np.concatenate([x_band1[:, :, :, np.newaxis]\n",
    "                          , x_band2[:, :, :, np.newaxis]\n",
    "                         , ((x_band1+x_band2)/2)[:, :, :, np.newaxis]], axis=-1)\n",
    "X_angle_test = np.array(test.inc_angle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Need to do: Augmentors should be functionalized, in order to keep X, y the same size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean = nd.array([-20.655821,-26.320704,-23.488279])\n",
    "std = nd.array([5.200841,3.3955173,3.8151529])\n",
    "normalizer = image.ColorNormalizeAug(mean, std)\n",
    "flip = image.HorizontalFlipAug(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train_new = [normalizer(nd.array(X_train[i])) for i in range(1604)]    #normalize\n",
    "X_train_new.extend([flip(X_train_new[i]) for i in range(1604)])     #flip\n",
    "y_train_new = np.append(y_train, y_train)  # y_train\n",
    "X_test_new = [normalizer(nd.array(X_test[i])) for i in range(8424)] #X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#resize: \n",
    "X_train_new = [nd.transpose(X_train_new[i],(2,0,1)) for i in range(len(X_train_new))]\n",
    "X_test_new = [nd.transpose(X_test_new[i],(2,0,1)) for i in range(len(X_test_new))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cpu(0)"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mxnet as mx\n",
    "try:\n",
    "    ctx = mx.gpu()\n",
    "    _ = nd.zeros((1,), ctx=ctx)\n",
    "except:\n",
    "    ctx = mx.cpu()\n",
    "ctx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model: ResNet 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mxnet.gluon import nn\n",
    "from mxnet import init\n",
    "\n",
    "\n",
    "class Residual(nn.HybridBlock):\n",
    "    def __init__(self, channels, same_shape=True, **kwargs):\n",
    "        super(Residual, self).__init__(**kwargs)\n",
    "        self.same_shape = same_shape\n",
    "        with self.name_scope():\n",
    "            strides = 1 if same_shape else 2\n",
    "            self.conv1 = nn.Conv2D(channels, kernel_size=3, padding=1, strides=strides)\n",
    "            self.bn1 = nn.BatchNorm()\n",
    "            self.conv2 = nn.Conv2D(channels, kernel_size=3, padding=1)\n",
    "            self.bn2 = nn.BatchNorm()\n",
    "            if not same_shape:\n",
    "                self.conv3 = nn.Conv2D(channels, kernel_size=1, strides=strides)\n",
    "\n",
    "    def hybrid_forward(self, F, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        if not self.same_shape:\n",
    "            x = self.conv3(x)\n",
    "        return F.relu(out + x)\n",
    "\n",
    "\n",
    "class ResNet(nn.HybridBlock):\n",
    "    def __init__(self, num_classes, verbose=False, **kwargs):\n",
    "        super(ResNet, self).__init__(**kwargs)\n",
    "        self.verbose = verbose\n",
    "        with self.name_scope():\n",
    "            net = self.net = nn.HybridSequential()\n",
    "            # block 1\n",
    "            net.add(nn.Conv2D(channels=32, kernel_size=3, strides=1, padding=1))\n",
    "            net.add(nn.BatchNorm())\n",
    "            net.add(nn.Activation(activation='relu'))\n",
    "            # block 2\n",
    "            for _ in range(3):\n",
    "                net.add(Residual(channels=32))\n",
    "            # block 3\n",
    "            net.add(Residual(channels=64, same_shape=False))\n",
    "            for _ in range(2):\n",
    "                net.add(Residual(channels=64))\n",
    "            # block 4\n",
    "            net.add(Residual(channels=128, same_shape=False))\n",
    "            for _ in range(2):\n",
    "                net.add(Residual(channels=128))\n",
    "            # block 5\n",
    "            net.add(nn.AvgPool2D(pool_size=8))\n",
    "            net.add(nn.Flatten())\n",
    "            net.add(nn.Dense(num_classes, activation='sigmoid'))\n",
    "\n",
    "    def hybrid_forward(self, F, x):\n",
    "        out = x\n",
    "        for i, b in enumerate(self.net):\n",
    "            out = b(out)\n",
    "            if self.verbose:\n",
    "                print('Block %d output: %s'%(i+1, out.shape))\n",
    "        return out\n",
    "\n",
    "\n",
    "def get_net(ctx):\n",
    "    num_outputs = 2\n",
    "    net = ResNet(num_outputs)\n",
    "    net.initialize(ctx=ctx, init=init.Xavier())\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_new, y_train_new, test_size=0.2, random_state=66)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hanfeimao/anaconda/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2010: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# for next use of angle:\n",
    "X_train, X_valid, X_angle_train, X_angle_valid, y_train, y_valid = train_test_split(X_train_all\n",
    "                    ,X_angle_train, y_train, random_state=66, train_size=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_ds = [(X_train[i],y_train[i]) for i in range(len(y_train))]\n",
    "valid_ds = [(X_test[i],y_test[i]) for i in range(len(y_test))]\n",
    "train_valid_ds = [(X_train_new[i], y_train_new[i]) for i in range(len(y_train_new))]\n",
    "test_ds = [(X_test_new[i], 0) for i in range(len(X_test_new))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mxnet import gluon\n",
    "batch_size = 128\n",
    "loader = gluon.data.DataLoader\n",
    "train_data = loader(train_ds, batch_size, shuffle=True, last_batch='keep')\n",
    "valid_data = loader(valid_ds, batch_size, shuffle=True, last_batch='keep')\n",
    "train_valid_data = loader(train_valid_ds, batch_size, shuffle=True, last_batch='keep')\n",
    "test_data = loader(test_ds, batch_size, shuffle = False, last_batch='keep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "def train(net, train_data, valid_data, num_epochs, lr, wd, ctx, lr_period, lr_decay):\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr, 'momentum': 0.9, 'wd': wd})\n",
    "\n",
    "    prev_time = datetime.datetime.now()\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0.0\n",
    "        train_acc = 0.0\n",
    "        if epoch > 0 and epoch % lr_period == 0:\n",
    "            trainer.set_learning_rate(trainer.learning_rate * lr_decay)\n",
    "        for data, label in train_data:\n",
    "            label = label.as_in_context(ctx)\n",
    "            with autograd.record():\n",
    "                output = net(data.as_in_context(ctx))\n",
    "                loss = softmax_cross_entropy(output, label)\n",
    "            loss.backward()\n",
    "            trainer.step(batch_size)\n",
    "            train_loss += nd.mean(loss).asscalar()\n",
    "            train_acc += accuracy(output, label)\n",
    "        cur_time = datetime.datetime.now()\n",
    "        h, remainder = divmod((cur_time - prev_time).seconds, 3600)\n",
    "        m, s = divmod(remainder, 60)\n",
    "        time_str = \"Time %02d:%02d:%02d\" % (h, m, s)\n",
    "        if valid_data is not None:\n",
    "            valid_acc = evaluate_accuracy(valid_data, net, ctx)\n",
    "            epoch_str = (\"Epoch %d. Loss: %f, Train acc %f, Valid acc %f, \"\n",
    "                         % (epoch, train_loss / len(train_data),\n",
    "                            train_acc / len(train_data), valid_acc))\n",
    "        else:\n",
    "            epoch_str = (\"Epoch %d. Loss: %f, Train acc %f, \"\n",
    "                         % (epoch, train_loss / len(train_data),\n",
    "                            train_acc / len(train_data)))\n",
    "        prev_time = cur_time\n",
    "        print(epoch_str + time_str + ', lr ' + str(trainer.learning_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(output, label):\n",
    "    return np.mean(output.asnumpy().argmax(axis=1)==label.asnumpy())\n",
    "\n",
    "def evaluate_accuracy(data_iterator, net, ctx=[mx.cpu()]):\n",
    "    if isinstance(ctx, mx.Context):\n",
    "        ctx = [ctx]\n",
    "    acc = nd.array([0])\n",
    "    n = 0.\n",
    "    if isinstance(data_iterator, mx.io.MXDataIter):\n",
    "        data_iterator.reset()\n",
    "    for batch in data_iterator:\n",
    "        data, label, batch_size = _get_batch(batch, ctx)\n",
    "        for X, y in zip(data, label):\n",
    "            acc += nd.array([np.sum(net(X).asnumpy().argmax(axis=1)==y.asnumpy())]).copyto(mx.cpu())\n",
    "        acc.wait_to_read() # don't push too many operators into backend\n",
    "        n += batch_size\n",
    "    return acc.asscalar() / n\n",
    "\n",
    "def _get_batch(batch, ctx):\n",
    "    \"\"\"return data and label on ctx\"\"\"\n",
    "    if isinstance(batch, mx.io.DataBatch):\n",
    "        data = batch.data[0]\n",
    "        label = batch.label[0]\n",
    "    else:\n",
    "        data, label = batch\n",
    "    return (gluon.utils.split_and_load(data, ctx),\n",
    "            gluon.utils.split_and_load(label, ctx),\n",
    "            data.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Loss: 1.736137, Train acc 0.576141, Valid acc 0.552960, Time 00:05:41, lr 0.05\n",
      "Epoch 1. Loss: 0.581291, Train acc 0.683036, Valid acc 0.641745, Time 00:05:55, lr 0.05\n",
      "Epoch 2. Loss: 0.640282, Train acc 0.699281, Valid acc 0.714953, Time 00:05:54, lr 0.05\n",
      "Epoch 3. Loss: 0.719692, Train acc 0.715650, Valid acc 0.735202, Time 00:05:55, lr 0.05\n",
      "Epoch 4. Loss: 0.556444, Train acc 0.760665, Valid acc 0.693146, Time 00:05:55, lr 0.05\n",
      "Epoch 5. Loss: 0.597260, Train acc 0.765625, Valid acc 0.738318, Time 00:05:55, lr 0.05\n",
      "Epoch 6. Loss: 0.429417, Train acc 0.786582, Valid acc 0.641745, Time 00:05:54, lr 0.05\n",
      "Epoch 7. Loss: 0.376101, Train acc 0.813864, Valid acc 0.794393, Time 00:05:56, lr 0.05\n",
      "Epoch 8. Loss: 0.559733, Train acc 0.796007, Valid acc 0.806854, Time 00:05:54, lr 0.05\n",
      "Epoch 9. Loss: 0.371577, Train acc 0.821305, Valid acc 0.850467, Time 00:05:54, lr 0.05\n",
      "Epoch 10. Loss: 0.322743, Train acc 0.861979, Valid acc 0.644860, Time 00:05:54, lr 0.05\n",
      "Epoch 11. Loss: 0.307963, Train acc 0.866815, Valid acc 0.827103, Time 00:05:55, lr 0.05\n",
      "Epoch 12. Loss: 0.299702, Train acc 0.868180, Valid acc 0.870717, Time 00:05:54, lr 0.05\n",
      "Epoch 13. Loss: 0.271240, Train acc 0.874380, Valid acc 0.749221, Time 00:05:54, lr 0.05\n",
      "Epoch 14. Loss: 0.287415, Train acc 0.876240, Valid acc 0.883178, Time 00:05:54, lr 0.05\n",
      "Epoch 15. Loss: 0.260225, Train acc 0.884673, Valid acc 0.859813, Time 00:05:53, lr 0.05\n",
      "Epoch 16. Loss: 0.271451, Train acc 0.885417, Valid acc 0.834891, Time 00:05:52, lr 0.05\n",
      "Epoch 17. Loss: 0.291405, Train acc 0.876488, Valid acc 0.677570, Time 00:05:54, lr 0.05\n",
      "Epoch 18. Loss: 0.299061, Train acc 0.867188, Valid acc 0.866044, Time 00:05:54, lr 0.05\n",
      "Epoch 19. Loss: 0.219709, Train acc 0.906250, Valid acc 0.836449, Time 00:05:54, lr 0.05\n",
      "Epoch 20. Loss: 0.232494, Train acc 0.905134, Valid acc 0.819315, Time 00:05:56, lr 0.05\n",
      "Epoch 21. Loss: 0.233272, Train acc 0.894717, Valid acc 0.876947, Time 00:05:57, lr 0.05\n",
      "Epoch 22. Loss: 0.226689, Train acc 0.908358, Valid acc 0.858255, Time 00:05:55, lr 0.05\n",
      "Epoch 23. Loss: 0.191022, Train acc 0.920759, Valid acc 0.903427, Time 00:05:56, lr 0.05\n",
      "Epoch 24. Loss: 0.174677, Train acc 0.933408, Valid acc 0.897196, Time 00:05:53, lr 0.05\n",
      "Epoch 25. Loss: 0.350302, Train acc 0.901042, Valid acc 0.911215, Time 00:05:55, lr 0.005000000000000001\n",
      "Epoch 26. Loss: 0.186356, Train acc 0.919395, Valid acc 0.904984, Time 00:05:54, lr 0.005000000000000001\n",
      "Epoch 27. Loss: 0.144280, Train acc 0.943824, Valid acc 0.912773, Time 00:05:54, lr 0.005000000000000001\n",
      "Epoch 28. Loss: 0.138196, Train acc 0.948289, Valid acc 0.911215, Time 00:05:54, lr 0.005000000000000001\n",
      "Epoch 29. Loss: 0.134302, Train acc 0.949777, Valid acc 0.911215, Time 00:05:54, lr 0.005000000000000001\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "learning_rate = 0.05\n",
    "weight_decay = 1e-4\n",
    "lr_period = 25\n",
    "lr_decay = 0.1\n",
    "\n",
    "net = get_net(ctx)\n",
    "net.hybridize()\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, weight_decay, ctx, lr_period, lr_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Loss: 0.595225, Train acc 0.691707, Time 00:07:08, lr 0.05\n",
      "Epoch 1. Loss: 0.472683, Train acc 0.839844, Time 00:07:00, lr 0.05\n",
      "Epoch 2. Loss: 0.481922, Train acc 0.828125, Time 00:06:56, lr 0.05\n",
      "Epoch 3. Loss: 0.455753, Train acc 0.855168, Time 00:07:19, lr 0.05\n",
      "Epoch 4. Loss: 0.432268, Train acc 0.877704, Time 00:07:04, lr 0.05\n",
      "Epoch 5. Loss: 0.435757, Train acc 0.872296, Time 00:07:15, lr 0.05\n",
      "Epoch 6. Loss: 0.412084, Train acc 0.898438, Time 00:07:09, lr 0.05\n",
      "Epoch 7. Loss: 0.415136, Train acc 0.895132, Time 00:07:09, lr 0.05\n",
      "Epoch 8. Loss: 0.409925, Train acc 0.896635, Time 00:07:08, lr 0.05\n",
      "Epoch 9. Loss: 0.399246, Train acc 0.911959, Time 00:07:08, lr 0.05\n",
      "Epoch 10. Loss: 0.395775, Train acc 0.913161, Time 00:07:06, lr 0.05\n",
      "Epoch 11. Loss: 0.395223, Train acc 0.917668, Time 00:07:57, lr 0.05\n",
      "Epoch 12. Loss: 0.407336, Train acc 0.901743, Time 00:08:00, lr 0.05\n",
      "Epoch 13. Loss: 0.385708, Train acc 0.925180, Time 00:07:57, lr 0.05\n",
      "Epoch 14. Loss: 0.369409, Train acc 0.943510, Time 00:07:24, lr 0.05\n",
      "Epoch 15. Loss: 0.388936, Train acc 0.925180, Time 00:07:39, lr 0.005000000000000001\n",
      "Epoch 16. Loss: 0.357788, Train acc 0.956130, Time 00:07:27, lr 0.005000000000000001\n",
      "Epoch 17. Loss: 0.351429, Train acc 0.963341, Time 00:07:17, lr 0.005000000000000001\n",
      "Epoch 18. Loss: 0.358878, Train acc 0.956731, Time 00:07:10, lr 0.005000000000000001\n",
      "Epoch 19. Loss: 0.350879, Train acc 0.963041, Time 00:07:07, lr 0.005000000000000001\n",
      "Epoch 20. Loss: 0.342840, Train acc 0.973257, Time 00:07:10, lr 0.005000000000000001\n",
      "Epoch 21. Loss: 0.350667, Train acc 0.965745, Time 00:07:09, lr 0.005000000000000001\n",
      "Epoch 22. Loss: 0.342428, Train acc 0.972957, Time 00:07:06, lr 0.005000000000000001\n",
      "Epoch 23. Loss: 0.336180, Train acc 0.980168, Time 00:07:06, lr 0.005000000000000001\n",
      "Epoch 24. Loss: 0.336529, Train acc 0.980168, Time 00:07:03, lr 0.005000000000000001\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 25\n",
    "learning_rate = 0.05\n",
    "weight_decay = 1e-4\n",
    "lr_period = 15\n",
    "lr_decay = 0.1\n",
    "\n",
    "net = get_net(ctx)\n",
    "net.hybridize()\n",
    "train(net, train_valid_data, None, num_epochs, learning_rate, weight_decay, ctx, lr_period, lr_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds = []\n",
    "for data, label in test_data:\n",
    "    output = net(data.as_in_context(ctx))\n",
    "    preds.extend(output[:,1].asnumpy())\n",
    "    \n",
    "df = pd.DataFrame({'id': test['id'], 'is_iceberg':preds})\n",
    "df['id'] =df['id'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'06565646' in list(df['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.to_csv('../submit/submission11050016.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mask = df.is_iceberg == 0\n",
    "column_name = 'is_iceberg'\n",
    "df.loc[mask, column_name] = 0.1\n",
    "mask = df.is_iceberg == 1\n",
    "df.loc[mask, column_name] = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
